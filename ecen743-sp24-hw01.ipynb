{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECEN743 Spring 2024 - Assignment 1\n",
    "## Tabular RL Algorithms\n",
    "\n",
    "In this assignment, you will solve the FrozenLake-v0 environment from [Gymnasium](https://gymnasium.farama.org/). You will be using this helper file to answer questions in your assignment. \n",
    "\n",
    "**Note that you do not need to start from the scratch. Only write your code between the following lines. Do not modify other parts.**  \n",
    "\"### YOUR CODE HERE\"  \n",
    "\"### END OF YOUR CODE\"\n",
    "\n",
    "## Introduction of the FrozenLake Environment\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake. The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment. Holes in the ice are distributed in set locations using a pre-determined map, and the player makes moves until they reach the goal or fall in a hole. The map is given below for your reference\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    \n",
    "    \n",
    "### Action Space\n",
    "The player/agent can take 4 discrete actions, in the range {0,3}\n",
    "* 0: Move left\n",
    "* 1: Move down\n",
    "* 2: Move right \n",
    "* 4: Move up\n",
    "\n",
    "\n",
    "### State Space\n",
    "The environment consists of 16 states. The state is a value representing the playerâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0).\n",
    "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "\n",
    "\n",
    "### Starting State\n",
    "The episode starts with the player in state [0] (location [0, 0]).\n",
    "\n",
    "\n",
    "### Rewards \n",
    "\n",
    "* Reach goal: +1\n",
    "* Reach hole: 0\n",
    "* Reach frozen: 0\n",
    "\n",
    "### Episode End\n",
    "The episode ends if the following happens:\n",
    "#### 1.Termination:\n",
    "* The player moves into a hole.\n",
    "* The player reaches the goal at max(nrow) * max(ncol) - 1 (location [max(nrow)-1, max(ncol)-1]).\n",
    "\n",
    "#### 2.Truncation:\n",
    "* The length of the episode is 100 for 4x4 environment.\n",
    "\n",
    "For more info refer to source: https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "### The Environment Parameters\n",
    "* Use discount factor, $\\gamma = 0.9$\n",
    "* The environment is slippery, ie., the transition kernel is stochastic.\n",
    "* The transition kernel P is a dictionary. \n",
    "* P[state][action] is tuples with (probability, nextstate, reward, terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following initializer. Make sure you can execute it without any error.**\n",
    "\n",
    "If you wish to finish this assignment using Google Colab. Uncomment the following commands and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None,map_name=\"4x4\", is_slippery=True)\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "def fancy_visual(value_func,policy_int):    \n",
    "    grid = 4    \n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = seaborn.diverging_palette(220, 10, as_cmap=True)\n",
    "    reshaped = np.reshape(value_func,(grid,grid))\n",
    "    seaborn.heatmap(reshaped, cmap=\"icefire\",vmax=1.1, robust = True,\n",
    "                square=True, xticklabels=grid+1, yticklabels=grid+1,\n",
    "                linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax, annot=True, fmt=\"f\")\n",
    "    counter = 0\n",
    "    for j in range(0, 4):\n",
    "        for i in range(0, 4):\n",
    "            if int(policy_int[counter]) == 1:\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2193', fontsize=12)\n",
    "            elif int(policy_int[counter]) == 3:\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2191', fontsize=12)\n",
    "            elif int(policy_int[counter]) == 0:\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2190', fontsize=12)\n",
    "            else:\n",
    "                plt.text(i+0.5, j+0.7, u'\\u2192', fontsize=12)\n",
    "            counter=counter+1\n",
    "\n",
    "    plt.title('Heatmap of policy iteration with value function values and directions')\n",
    "    print('Value Function',value_func)\n",
    "    print('Policy',policy_int)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-value iteration\n",
    "\n",
    "### Explanation of the parameters\n",
    "* `Q_prev` is the Q-value function from the previous iteration.\n",
    "* `Q_curr` is the Q-value function of current iteration. In each iteration, you need to use model, reward, and the Q-value function from previous iteration (`Q_prev`) to compute `Q_curr`.\n",
    "* You can use\n",
    "    * `n = np.array([n for (p,n,r,t) in env.P[s][a]])` to access the information of the possible next states if you pick action `a` in state `s`. **Note that for conciseness, `env.P[s][a]` omits entries that are not reachable from `(s,a)`;** \n",
    "    * `p = np.array([p for (p,n,r,t) in env.P[s][a]])` to access the transition probabilities at any state-action pair `(s,a)`. For example, in state `4`, if you choose to move right (`a=2`), then `env.P[4][2]` outputs:\n",
    "        ```\n",
    "        [(0.3333333333333333, 8, 0.0, False),\n",
    "         (0.3333333333333333, 5, 0.0, True),\n",
    "         (0.3333333333333333, 0, 0.0, False)]\n",
    "        ```\n",
    "        That is, there is a one third chance that we end up in either state `8`, `5`, or `0`. **This is very import since the indexing of this array is totally irrelavant to the indexing of your Q-value function array.** In this particular example, you need to update `Q_curr[4]` using `p[0]` and `Q_prev[8]`, `p[1]` and `Q_prev[5]`, `p[2]` and `Q_prev[0]` (Why?); \n",
    "    * `r = np.array([r for (p,n,r,t) in env.P[s][a]])` to access the reward.\n",
    "\n",
    "### Your Task\n",
    "1. Complete the Bellman update in Task 1 below. In particular, calculate the current Q-value function `Q_curr` using the previous Q-value function `Q_prev`.\n",
    "2. Compute the optimal value function `Vopt` and the optimal policy `Policyopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_actions = env.action_space.n\n",
    "no_of_states = env.observation_space.n\n",
    "\n",
    "epsilon = 1e-5\n",
    "\n",
    "\n",
    "Q_prev = np.array([[0.0]*no_of_actions for i in range(no_of_states)])\n",
    "Q_delta = []\n",
    "delta = 1\n",
    "while(delta > epsilon):\n",
    "    Q_curr = np.array([[0.0]*no_of_actions for i in range(no_of_states)])\n",
    "    # Task 1\n",
    "    ### YOUR CODE HERE\n",
    "    # Q_curr = []\n",
    "    ### END OF YOUR CODE\n",
    "    delta = np.linalg.norm(Q_curr.flatten() - Q_prev.flatten())\n",
    "    Q_delta.append(delta)\n",
    "    Q_prev = Q_curr\n",
    "\n",
    "# Task 2\n",
    "### YOUR CODE HERE\n",
    "# Vopt = 0\n",
    "# Policyopt = []\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Optimal Policy and Value function\n",
    "\n",
    "You do not need to modify the code below but you have to run it before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Value function: \")\n",
    "print(Vopt)\n",
    "print(\"Optimal Policy\")\n",
    "print(Policyopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Plot $||Q_k - Q_{k-1}||$\n",
    "\n",
    "You do not need to modify the code below but you have to run it before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(Q_delta)),Q_delta)\n",
    "plt.title(\"Q-value Iteration\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"$||Q_k - Q_{k-1}||$\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Heat map\n",
    "\n",
    "You do not need to modify the code below but you have to run it before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_visual(Vopt,Policyopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Evaluation  \n",
    "\n",
    "### 2a. linear system of equations\n",
    "\n",
    "### Hint: one way to do this\n",
    "1. Compute `P_opt` which is a $|\\mathcal{S}|\\times|\\mathcal{S}|$ matrix where the entry in the $i$-th row, $j$-th column represents the probability of going from state `i` to state `j` by executing the optimal policy obtained by QVI in Problem 1.\n",
    "\n",
    "2. Compute `r_opt` which is a $|\\mathcal{S}|$-dimensional vector whose $i$-th element is $$\\mathbb{E}_{a\\sim \\pi^*(\\cdot\\mid s)}\\mathbb{E}_{s'\\sim P(\\cdot\\mid s,a)}[r(s,a,s')] \\stackrel{(a)}{=} \\mathbb{E}_{s'\\sim P(\\cdot\\mid s,\\pi^*(s))}[r(s,\\pi^*(s),s')]$$, where $(a)$ is because $\\pi^*$ is deterministic.\n",
    "\n",
    "3. Recall the Bellman consistency equation, for any policy $\\pi$, we have $V^{\\pi} = (I-\\gamma P^{\\pi})^{-1} r^{\\pi}$. Rearrange the terms, we can look at the system of linear equations, $(I-\\gamma P^{\\pi}) V^{\\pi} = r^{\\pi}$, and solve for $V^\\pi$.\n",
    "\n",
    "4. For $\\pi_{\\mathrm{unif}}$, repeat the steps above but be careful since the policy is no longer deterministic. You need to do extra work in step 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal policy\n",
    "I = np.identity(no_of_states)\n",
    "P_opt = np.zeros((no_of_states,no_of_states))\n",
    "r_opt = np.zeros(no_of_states)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "     \n",
    "### END OF YOUR CODE\n",
    "\n",
    "# Uniform policy\n",
    "I = np.identity(no_of_states)\n",
    "P_unif = np.zeros((no_of_states,no_of_states))\n",
    "r_unif = np.zeros(no_of_states)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "### END OF YOUR CODE\n",
    "\n",
    "print(\"Value function under optimal policy: \")\n",
    "### YOUR CODE HERE\n",
    "# print your value function for the optimal policy\n",
    "### END OF YOUR CODE\n",
    "\n",
    "print(\"Value function under uniformly random policy:\")\n",
    "### YOUR CODE HERE\n",
    "# print your value function for the uniform policy\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Iterative Method\n",
    "\n",
    "Recall the Bellman consistency equation that for any policy $\\pi$, we have\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi(\\cdot\\mid s)}\\mathbb{E}_{s'\\sim P(\\cdot\\mid s,a)}[r(s,a,s') + \\gamma V^\\pi(s')].\n",
    "$$\n",
    "\n",
    "Please keep `epsilon` unchanged. Remember to update the `delta` in the while loop to reflect the current convergency of the contraction mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-10\n",
    "#optimal policy\n",
    "V_prev = np.array([0.0]*no_of_states)\n",
    "delta = 1\n",
    "while(delta > epsilon):\n",
    "    ### YOUR CODE HERE\n",
    "    # V_curr = ?\n",
    "    # delta = compare V_prev and V_curr\n",
    "    ### END OF YOUR CODE\n",
    "    V_prev = V_curr\n",
    "\n",
    "print(\"Value function under optimal policy: \")\n",
    "print(V_prev)\n",
    "\n",
    "#uniform policy\n",
    "V_prev = np.array([0.0]*no_of_states)\n",
    "delta = 1\n",
    "while(delta > epsilon):\n",
    "    ### YOUR CODE HERE\n",
    "    # V_curr = ?\n",
    "    # delta = compare V_prev and V_curr\n",
    "    ### END OF YOUR CODE\n",
    "    V_prev = V_curr\n",
    "    \n",
    "print(\"Value function under uniform policy: \")\n",
    "print(V_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer below.**  \n",
    "Answer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Iteration\n",
    "\n",
    "### Hint: one way to do this\n",
    "1. In the policy evaluation step, calculate `P_pol` matrix. This should be identical to Problem **2a**.\n",
    "2. Calculate `r_pol` which is a $|\\mathcal{S}|$-dimensional vector. You should have\n",
    "$r_\\mathrm{pol}[s] = \\mathbb{E}_{a\\sim\\pi_{\\mathrm{prev}}(\\cdot\\mid s)} \\mathbb{E}_{s'\\sim P(\\cdot\\mid s,a)} [r(s,a,s')]. $\n",
    "Note that this should be just one line. Refer to **Explanation of the parameters** in **Problem 1** for `p`, `n`, and `r`.  \n",
    "3. Compute `V_curr` using `P_pol` and `r_pol`. It represents the value function of the current policy $\\pi_k$.\n",
    "4. Compute new policy $\\pi_{k+1}$ using the value function of $\\pi_k$. In particular, you need to use `V_pol_prev` to compute `pi_curr`.\n",
    "\n",
    "**Note, for this problem, you can remove everything below and start from the scratch. **\n",
    "\n",
    "**However, you have to save your optimal policy as `piopt_politer` and your optimal value function as `Vopt_politer`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_prev = np.random.randint(0,no_of_actions,size=no_of_states)\n",
    "V_pi_delta = []\n",
    "V_pol_prev = np.array([0.0]*no_of_states)\n",
    "\n",
    "epsilon_o = 1e-5\n",
    "epsilon_i = 1e-10\n",
    "\n",
    "delta_o = 1\n",
    "while(delta_o > 0):\n",
    "    ### YOUR CODE HERE\n",
    "    # some prep work for policy evaluation\n",
    "    ###\n",
    "    delta_i = 1\n",
    "    while(delta_i > epsilon_i):\n",
    "        ### YOUR CODE HERE\n",
    "        # V_curr = r_pol + gamma*(P_pol @ V_pol_prev_i)\n",
    "        ### END OF YOUR CODE\n",
    "        # delta_i = compare V_curr and V_pol_prev_i\n",
    "        V_pol_prev_i = V_curr  # update while loop\n",
    "        \n",
    "    V_pol_prev = V_pol_prev_i\n",
    "    \n",
    "    pi_curr = np.zeros((no_of_states,), dtype=int)\n",
    "    ### YOUR CODE HERE\n",
    "    # design your own policy improvement steps\n",
    "    ### END OF YOUR CODE\n",
    "    # delta_o = compare pi_curr and pi_prev\n",
    "    #print(delta_o)\n",
    "    pi_prev = pi_curr  # update while loop\n",
    "\n",
    "plt.plot(range(len(V_pi_delta)),V_pi_delta)\n",
    "plt.title(\"Policy Iteration\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"$||V_{\\pi_k} - V_{\\pi_{k-1}}||$\")\n",
    "plt.show()\n",
    "\n",
    "# Remember to save your optimal policy as `piopt_politer` and your optimal value function as `Vopt_politer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Optimal Policy and Value function\n",
    "\n",
    "You do not need to modify the code below but you have to run it before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Value function: \")\n",
    "print(Vopt_politer)\n",
    "print(\"Optimal Policy\")\n",
    "print(piopt_politer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Compare the convergence of QVI and PI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer below.**  \n",
    "Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
